p8105_hw5_kr3145
================
Kallan Roan
2025-11-08

Load libraries

``` r
library(tidyverse)
```

    ## ── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
    ## ✔ dplyr     1.1.4     ✔ readr     2.1.5
    ## ✔ forcats   1.0.0     ✔ stringr   1.5.1
    ## ✔ ggplot2   3.5.2     ✔ tibble    3.3.0
    ## ✔ lubridate 1.9.4     ✔ tidyr     1.3.1
    ## ✔ purrr     1.1.0     
    ## ── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
    ## ✖ dplyr::filter() masks stats::filter()
    ## ✖ dplyr::lag()    masks stats::lag()
    ## ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors

Preset settings

``` r
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d
```

Set seed

``` r
set.seed(1)
```

## Problem 1

Create function

``` r
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
  
}
```

Create simulation for group size between 2 and 50

``` r
sim_results_df = 
  expand_grid(
    sample_size = 2:50, 
    iter = 1:10000
  ) |> 
  mutate(
    results = map(sample_size, bday_sim)
  ) |> 
  unnest(results)
```

Use simulation results to calculate probability and plot

``` r
sim_results_df |> 
  group_by(sample_size) |> 
  summarize(
    prob = mean(results)
  ) |> 
  ggplot(aes(x = sample_size, y = prob)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    x = "Group Size",
    y = "Probability"
  ) 
```

    ## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'

<img src="p8105_hw5_kr3145_files/figure-gfm/unnamed-chunk-6-1.png" width="90%" />

The plot shows that as the group size increases, the probability that at
least 2 people in the group will share a birthday will also increase.
The increase starts to plateau slightly as it gets to group size of 50.

## Problem 2

Create function for running t-test for means 0 to 6

``` r
t_test_fctn = function(m) {
  
  num_vec = rnorm(30, m, sd = 5)
  
  t_test = t.test(num_vec, mu = 0)
  
  t_test
  
}
```

Run simulation 5000 times, pulling estimate and p-value

``` r
sim_results_2_df = 
  expand_grid(
    mean = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    results = map(mean, t_test_fctn),
    results = map(results, broom::tidy)
  ) |> 
  unnest(results) |> 
  select(mean:p.value)
```

Plot \#1

``` r
sim_results_2_df |> 
  mutate(
    reject_null = p.value <= 0.05
  ) |> 
  group_by(mean) |> 
  summarize(
    power = mean(reject_null)
  ) |> 
  ggplot(aes(x = mean, y = power)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    x = "Effect Size",
    y = "Power"
  ) 
```

    ## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'

<img src="p8105_hw5_kr3145_files/figure-gfm/unnamed-chunk-9-1.png" width="90%" />

The plot shows that as effect size increases, power increases. However,
the increase does plateau at effect size of 4, where the power maintains
consistent at a value of 1.

Dataframe with average estimate of sample mean for all samples

``` r
all_results_df = 
  sim_results_2_df |> 
  group_by(mean) |> 
  summarize(
    avg_estimate_mean = mean(estimate),
    type = "All"
    ) 
```

Dataframe with average estimate of sample mean for only samples for
which null was rejected

``` r
rejected_null_df = 
  sim_results_2_df |> 
  filter(p.value <= 0.05) |> 
  group_by(mean) |> 
  summarize(
    avg_estimate_mean = mean(estimate),
    type = "Rejected Null"
  ) 
```

Create plot with results for all samples and for only samples where null
was rejected

``` r
bind_rows(all_results_df, rejected_null_df) |> 
  ggplot(aes(x = mean, y = avg_estimate_mean, color = type)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    title = "True mean vs Average Estimate of Mean",
    x = "True mean",
    y = "Average estimate of mean"
  ) 
```

    ## `geom_smooth()` using method = 'loess' and formula = 'y ~ x'

<img src="p8105_hw5_kr3145_files/figure-gfm/unnamed-chunk-12-1.png" width="90%" />

In general, the sample average of sample mean across tests for which the
null is rejected is approximately equal to the true mean. However, the
tests with a true mean of between 1 and 3 has a higher sample average of
sample mean for those with rejected null. This is the case because for
tests when null is rejected, sample mean is farther away from the true
mean. In this case, the average estimate of mean is larger than the true
mean. Since we are only keeping results where the null is rejected, only
values with more extreme values are included in the final output.

## Problem 3

Import data

``` r
homicide_df = 
  read_csv("data/homicide-data.csv") |>
  janitor::clean_names() 
```

    ## Rows: 52179 Columns: 12
    ## ── Column specification ────────────────────────────────────────────────────────
    ## Delimiter: ","
    ## chr (9): uid, victim_last, victim_first, victim_race, victim_age, victim_sex...
    ## dbl (3): reported_date, lat, lon
    ## 
    ## ℹ Use `spec()` to retrieve the full column specification for this data.
    ## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.

The raw data of `homicide_df` contains 52179 observations representing
each criminal homicide in 50 of the largest American cities. Demographic
information of victims, including `victim_last`, `victim_first`,
`victim_race`, `victim_age`, and `victim_sex`, are included. `city`,
`state`, `lat`, and `long` represent the location of homicide.
`disposition` represents how the homicide case was concluded.

Create city_state variable

``` r
homicide_df = 
  homicide_df |> 
  mutate(
    city_state = str_c(city, ", ", state)
  )
```

Summarize to obtain total homicides and unresolved homicdites per city

``` r
homicide_summary = 
  homicide_df |> 
  group_by(city_state) |> 
  summarize(
    total_hom = n(),
    unsolved_hom = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
    )
```

Baltimore, MD

``` r
baltimore_hom = 
  homicide_summary |> 
  filter(city_state == "Baltimore, MD") |> 
  with(prop.test(unsolved_hom, total_hom))

baltimore_hom |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high) |> 
  knitr::kable(digits = 3)
```

| estimate | conf.low | conf.high |
|---------:|---------:|----------:|
|    0.646 |    0.628 |     0.663 |

All cities

``` r
homicide_prop_conf = 
  homicide_summary |> 
  mutate(
    test_result = map2(unsolved_hom, total_hom, ~ prop.test(.x, .y)),
    result = map(test_result, broom::tidy)
  ) |> 
  unnest(result) |> 
  select(city_state, estimate, conf.low, conf.high) 
```

    ## Warning: There was 1 warning in `mutate()`.
    ## ℹ In argument: `test_result = map2(unsolved_hom, total_hom, ~prop.test(.x,
    ##   .y))`.
    ## Caused by warning in `prop.test()`:
    ## ! Chi-squared approximation may be incorrect

``` r
homicide_prop_conf |> 
  knitr::kable(digits = 3)
```

| city_state         | estimate | conf.low | conf.high |
|:-------------------|---------:|---------:|----------:|
| Albuquerque, NM    |    0.386 |    0.337 |     0.438 |
| Atlanta, GA        |    0.383 |    0.353 |     0.415 |
| Baltimore, MD      |    0.646 |    0.628 |     0.663 |
| Baton Rouge, LA    |    0.462 |    0.414 |     0.511 |
| Birmingham, AL     |    0.434 |    0.399 |     0.469 |
| Boston, MA         |    0.505 |    0.465 |     0.545 |
| Buffalo, NY        |    0.612 |    0.569 |     0.654 |
| Charlotte, NC      |    0.300 |    0.266 |     0.336 |
| Chicago, IL        |    0.736 |    0.724 |     0.747 |
| Cincinnati, OH     |    0.445 |    0.408 |     0.483 |
| Columbus, OH       |    0.530 |    0.500 |     0.560 |
| Dallas, TX         |    0.481 |    0.456 |     0.506 |
| Denver, CO         |    0.542 |    0.485 |     0.598 |
| Detroit, MI        |    0.588 |    0.569 |     0.608 |
| Durham, NC         |    0.366 |    0.310 |     0.426 |
| Fort Worth, TX     |    0.464 |    0.422 |     0.507 |
| Fresno, CA         |    0.347 |    0.305 |     0.391 |
| Houston, TX        |    0.507 |    0.489 |     0.526 |
| Indianapolis, IN   |    0.449 |    0.422 |     0.477 |
| Jacksonville, FL   |    0.511 |    0.482 |     0.540 |
| Kansas City, MO    |    0.408 |    0.380 |     0.437 |
| Las Vegas, NV      |    0.414 |    0.388 |     0.441 |
| Long Beach, CA     |    0.413 |    0.363 |     0.464 |
| Los Angeles, CA    |    0.490 |    0.469 |     0.511 |
| Louisville, KY     |    0.453 |    0.412 |     0.495 |
| Memphis, TN        |    0.319 |    0.296 |     0.343 |
| Miami, FL          |    0.605 |    0.569 |     0.640 |
| Milwaukee, wI      |    0.361 |    0.333 |     0.391 |
| Minneapolis, MN    |    0.511 |    0.459 |     0.563 |
| Nashville, TN      |    0.362 |    0.329 |     0.398 |
| New Orleans, LA    |    0.649 |    0.623 |     0.673 |
| New York, NY       |    0.388 |    0.349 |     0.427 |
| Oakland, CA        |    0.536 |    0.504 |     0.569 |
| Oklahoma City, OK  |    0.485 |    0.447 |     0.524 |
| Omaha, NE          |    0.413 |    0.365 |     0.463 |
| Philadelphia, PA   |    0.448 |    0.430 |     0.466 |
| Phoenix, AZ        |    0.551 |    0.518 |     0.584 |
| Pittsburgh, PA     |    0.534 |    0.494 |     0.573 |
| Richmond, VA       |    0.263 |    0.223 |     0.308 |
| Sacramento, CA     |    0.370 |    0.321 |     0.421 |
| San Antonio, TX    |    0.429 |    0.395 |     0.463 |
| San Bernardino, CA |    0.618 |    0.558 |     0.675 |
| San Diego, CA      |    0.380 |    0.335 |     0.426 |
| San Francisco, CA  |    0.507 |    0.468 |     0.545 |
| Savannah, GA       |    0.467 |    0.404 |     0.532 |
| St. Louis, MO      |    0.540 |    0.515 |     0.564 |
| Stockton, CA       |    0.599 |    0.552 |     0.645 |
| Tampa, FL          |    0.457 |    0.388 |     0.527 |
| Tulsa, AL          |    0.000 |    0.000 |     0.945 |
| Tulsa, OK          |    0.331 |    0.293 |     0.371 |
| Washington, DC     |    0.438 |    0.411 |     0.465 |

Create a plot

``` r
homicide_prop_conf |> 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |> 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    title = "Proportion of Unsolved Homicides by Location",
    x = "Location",
    y = "Proportion of Unsolved Homicides"
  ) 
```

<img src="p8105_hw5_kr3145_files/figure-gfm/unnamed-chunk-18-1.png" width="90%" />
