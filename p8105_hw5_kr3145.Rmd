---
title: "p8105_hw5_kr3145"
author: "Kallan Roan"
date: "2025-11-08"
output: github_document
---

Load libraries 

```{r}
library(tidyverse)
```

Preset settings

```{r}
knitr::opts_chunk$set(
  fig.width = 6,
  fig.asp = .6,
  out.width = "90%"
)

theme_set(theme_minimal() + theme(legend.position = "bottom"))

options(
  ggplot2.continuous.colour = "viridis",
  ggplot2.continuous.fill = "viridis"
)

scale_colour_discrete = scale_colour_viridis_d
scale_fill_discrete = scale_fill_viridis_d

```

Set seed

```{r}
set.seed(1)
```


## Problem 1

Create function

```{r}
bday_sim = function(n_room) {
  
  birthdays = sample(1:365, n_room, replace = TRUE)
  
  repeated_bday = length(unique(birthdays)) < n_room
  
  repeated_bday
  
}
```

Create simulation for group size between 2 and 50

```{r}
sim_results_df = 
  expand_grid(
    sample_size = 2:50, 
    iter = 1:10000
  ) |> 
  mutate(
    results = map(sample_size, bday_sim)
  ) |> 
  unnest(results)
```

Use simulation results to calculate probability and plot

```{r}
sim_results_df |> 
  group_by(sample_size) |> 
  summarize(
    prob = mean(results)
  ) |> 
  ggplot(aes(x = sample_size, y = prob)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    x = "Group Size",
    y = "Probability"
  ) 
```

The plot shows that as the group size increases, the probability that at least 2 people in the group will share a birthday will also increase. The increase starts to plateau slightly as it gets to group size of 50. 

## Problem 2

Create function for running t-test for means 0 to 6

```{r}
t_test_fctn = function(m) {
  
  num_vec = rnorm(30, m, sd = 5)
  
  t_test = t.test(num_vec, mu = 0)
  
  t_test
  
}
```

Run simulation 5000 times, pulling estimate and p-value

```{r}
sim_results_2_df = 
  expand_grid(
    mean = 0:6,
    iter = 1:5000
  ) |> 
  mutate(
    results = map(mean, t_test_fctn),
    results = map(results, broom::tidy)
  ) |> 
  unnest(results) |> 
  select(mean:p.value)
```

Plot #1

```{r}
sim_results_2_df |> 
  mutate(
    reject_null = p.value <= 0.05
  ) |> 
  group_by(mean) |> 
  summarize(
    power = mean(reject_null)
  ) |> 
  ggplot(aes(x = mean, y = power)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    x = "Effect Size",
    y = "Power"
  ) 
```

The plot shows that as effect size increases, power increases. However, the increase does plateau at effect size of 4, where the power maintains consistent at a value of 1. 

Dataframe with average estimate of sample mean for all samples

```{r}
all_results_df = 
  sim_results_2_df |> 
  group_by(mean) |> 
  summarize(
    avg_estimate_mean = mean(estimate),
    type = "All"
    ) 
```

Dataframe with average estimate of sample mean for only samples for which null was rejected

```{r}
rejected_null_df = 
  sim_results_2_df |> 
  filter(p.value <= 0.05) |> 
  group_by(mean) |> 
  summarize(
    avg_estimate_mean = mean(estimate),
    type = "Rejected Null"
  ) 
```

Create plot with results for all samples and for only samples where null was rejected

```{r}
bind_rows(all_results_df, rejected_null_df) |> 
  ggplot(aes(x = mean, y = avg_estimate_mean, color = type)) +
  geom_point() +
  geom_smooth(se = FALSE) +
  labs(
    title = "True mean vs Average Estimate of Mean",
    x = "True mean",
    y = "Average estimate of mean"
  ) 
```

In general, the sample average of sample mean across tests for which the null is rejected is approximately equal to the true mean. However, the tests with a true mean of between 1 and 3 has a higher sample average of sample mean for those with rejected null. This is the case because for tests when null is rejected, sample mean is farther away from the true mean. In this case, the average estimate of mean is larger than the true mean. Since we are only keeping results where the null is rejected, only values with more extreme values are included in the final output. 

## Problem 3

Import data

```{r}
homicide_df = 
  read_csv("data/homicide-data.csv") |>
  janitor::clean_names() 
```

The raw data of `homicide_df` contains `r nrow(homicide_df)` observations representing each criminal homicide in 50 of the largest American cities. Demographic information of victims, including `victim_last`, `victim_first`, `victim_race`, `victim_age`, and `victim_sex`, are included. `city`, `state`, `lat`, and `long` represent the location of homicide. `disposition` represents how the homicide case was concluded. 

Create city_state variable

```{r}
homicide_df = 
  homicide_df |> 
  mutate(
    city_state = str_c(city, ", ", state)
  )
```

Summarize to obtain total homicides and unresolved homicdites per city

```{r}
homicide_summary = 
  homicide_df |> 
  group_by(city_state) |> 
  summarize(
    total_hom = n(),
    unsolved_hom = sum(disposition %in% c("Closed without arrest", "Open/No arrest"))
    )
```

Baltimore, MD

```{r}
baltimore_hom = 
  homicide_summary |> 
  filter(city_state == "Baltimore, MD") |> 
  with(prop.test(unsolved_hom, total_hom))

baltimore_hom |> 
  broom::tidy() |> 
  select(estimate, conf.low, conf.high) |> 
  knitr::kable(digits = 3)
```

All cities

```{r}
homicide_prop_conf = 
  homicide_summary |> 
  mutate(
    test_result = map2(unsolved_hom, total_hom, ~ prop.test(.x, .y)),
    result = map(test_result, broom::tidy)
  ) |> 
  unnest(result) |> 
  select(city_state, estimate, conf.low, conf.high) 

homicide_prop_conf |> 
  knitr::kable(digits = 3)
```

Create a plot

```{r}
homicide_prop_conf |> 
  mutate(
    city_state = fct_reorder(city_state, estimate)
  ) |> 
  ggplot(aes(x = city_state, y = estimate)) +
  geom_point() +
  geom_errorbar(aes(ymin = conf.low, ymax = conf.high)) +
  theme(axis.text.x = element_text(angle = 45, hjust = 1)) +
  labs(
    title = "Proportion of Unsolved Homicides by Location",
    x = "Location",
    y = "Proportion of Unsolved Homicides"
  ) 
```

